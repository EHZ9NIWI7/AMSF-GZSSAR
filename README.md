# AMSF-GZSSAR
This is the official implementation of the paper:  **"Multi-View Knowledge Guided Semantic Prototype Learning for Generalized Zero-Shot Action Recognition"**. 
(*IEEE Transactions on Multimedia*.)
ðŸ‘‰ [Paper Link](https://ieeexplore.ieee.org/document/11194256)

![GZSSAR](/figure/gzssar.png)
<!-- Generalized Zero-Shot Skeleton-Based Action Recognition (**GZSSAR**) -->

## Approach
The Attentional Multi-view Semantic Fusion (**AMSF**) model is an improved version of [the MSF model](https://github.com/EHZ9NIWI7/MSF-GZSSAR), incorporating action descriptions generated by GPT-4 and a multi-semantic fusion strategy based on the multi-head attention.

![AMSF](/figure/ova.png)
<!-- The AMSF model -->

## Dependencies
<!-- ## Dependencies -->
* Python >= 3.8.13
* Torch >= 1.12.1
* Scikit-Learn

## Dataset
We evaluate our method on the following datasets:
- [NTU-60 & NTU-120](https://rose1.ntu.edu.sg/dataset/actionRecognition/)
- [PKU-MMD](https://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html)

## Data Preparation
To run this code, you need to place the skeleton features in [./data/sk_feat/](./data/sk_feat/), the semantic features in [./data/text_feat/](./data/text_feat/), and the dataset split settings in [./data/splits/](./data/splits/). All the data used in our experiments can be downloaded [here](https://drive.google.com/drive/folders/1LPWVzs5iUsUvKufBsDbqFTv1aQGQ8CXW?usp=drive_link). 

If you wish to use other visual features, please refer to the [./GZSSAR/dataset.py](./GZSSAR/dataset.py) file and follow the saving format used in [./data/sk_feat/](./data/sk_feat/). 
To generate semantic features using text encoders from CLIP or Long-CLIP, please refer to the [./gen_text_feat.py](./gen_text_feat.py) file. 
If you want to customize the split settings, please refer to the [./gen_split.py](./gen_split.py) file.

## Running
Please run this code via <code>main.py</code>. The arguments can be specified either through the [YAML](./config/MHA.yaml) file (in [./config/](./config/)) or the command line. (Arguments specified via the command line will take precedence over those defined in the YAML file.)

```bash
python main.py -c MHA.yaml
python main.py -g 0
```

If you want to perform step-by-step training/testing, or use only the ZSL mode, please modify the <code>mode</code> parameter.

```bash
python main.py -m zsl
```

## Citation
```bibtex
@ARTICLE{11194256,
  author={Li, Ming-Zhe and Jia, Zhen and Zhang, Zhang and Li, Yaoning and Ma, Zhanyu and Wang, Liang},
  journal={IEEE Transactions on Multimedia}, 
  title={Multi-View Knowledge Guided Semantic Prototype Learning for Generalized Zero-Shot Action Recognition}, 
  year={2025},
  volume={27},
  number={},
  pages={9735-9748},
  keywords={Semantics;Skeleton;Feature extraction;Prototypes;Training;Hands;Visualization;Annotations;Zero shot learning;Data mining;Skeleton-based action recognition;generalized zero-shot learning;action descriptions;semantic fusion},
  doi={10.1109/TMM.2025.3618570}}

